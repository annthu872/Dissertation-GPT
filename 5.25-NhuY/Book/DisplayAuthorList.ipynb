{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt_raw(json_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in json_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt(json_data, file_name):\n",
    "    filtered_data = {k: v for k, v in json_data.items() if v.get(\"reflect\") == \"yes\"}\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in filtered_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_8=\"\"\"\n",
    "I want you to act as a software tester.\n",
    "Your task is to read the test scenario's name and the corresponding use case specification to base on those information for generateing test steps for test cases and their following expected result.\n",
    "Return the test cases in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"explanation\": \"Why do you create this test case? How does this test case related to the test scenario inputed?\",\n",
    "  ],\n",
    "}\n",
    "If there are more than one test case for this scenario, continue writing other test case in this form.\n",
    "\n",
    "Rules for generating test steps:\n",
    "- Describe the test step clearly to make sure each test case is independent, tester do not need to read other information (example: other test case, use case specification) to know how to do that step.\n",
    "- Avoid references to other test cases or instructions like \"do as mentioned.\"\n",
    "- If the test case need to be repeated to test with different order, data or case, seperate them to be distinct test cases.\n",
    "- If the scenario is about testing the displation and there is no flow directly cover that scenario, use only the basic (or main) flow to test it.\n",
    "- If there are use cases mentioned in extended or included use case, create test case combine use cases, Try to find the connection point of use cases for combination. \n",
    "- For test scenarios mentioning navigation in the name, only produce test cases related to the specified navigation method. \n",
    "(Example: \"Scenario: User navigates to a page by navbar\", only produce a test case of user navigates to that page by navbar even though the use case description has many way to navigate to that page)\n",
    "- For test scenarios not mentioning navigation in the name, do not include any navigation test cases.\n",
    "- Generate test cases that directly match the scenario name. Choose only one flow to cover the scenario.\n",
    "- Ensure all actions and objectives match the scenario name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_REF=\"\"\"\n",
    "Given a test scenario and test cases to test that given test scenario.\n",
    "Mark if test case can test the given test scenario or not through test steps, expected output, objective (although if it test other use case path or flow, if it is not used to test the given test scenario.)\n",
    "And give explanation why you think the resulted test case reflect the given test scenario or not. \n",
    "Your response should keep the format of the inserted test cases.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"reflect\": \"yes/no\",\n",
    "    \"explanation\": \"explain why you think this test case reflect the given test scenario or not\",\n",
    "  ],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "\"1. Display First Page of Authors\",\n",
    "\"2. Navigate to Next Page of Authors\",\n",
    "\"3. Navigate to a Specific Page of Authors\",\n",
    "\"4. Last Page of Authors Displayed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uc = \"\"\"\n",
    "Display the list of authors\n",
    "Description\n",
    "The page is designed to list all the authors.\n",
    "\n",
    "Main scenario:\n",
    "User selects item \"Authors\";\n",
    "Application displays the list of the authors (up to 10 items on each page);\n",
    "To see the next 10 records of the authors in the database, the user needs to click the \"Next\" button (below the records in the pagination block) or the following number of the page the user wants to see.\n",
    "\n",
    "The list displays the following columns:\n",
    "id - unique author number\n",
    "Name\n",
    "Bio - author's biography\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios:\n",
    "    i = 1\n",
    "    while(i<=1):\n",
    "        try: \n",
    "            promptTestCase = [\n",
    "            { \"role\": \"system\", \"content\": SYSTEM_PROMPT_8},\n",
    "            { \"role\": \"user\", \"content\": scenario + \"\\n\" + prompt_uc}\n",
    "            ]\n",
    "            gpt_response = ask(promptTestCase, client, model)\n",
    "            json_data = json.loads(gpt_response)\n",
    "            write_json_to_txt_raw(json_data, scenario + \"-Raw\" + str(i) + \".txt\")\n",
    "\n",
    "            promptSelfRef = [\n",
    "                { \"role\": \"system\", \"content\": SELF_REF},\n",
    "                { \"role\": \"user\", \"content\": scenario + \"\\n\" + gpt_response}\n",
    "            ]\n",
    "            final_response = ask(promptSelfRef, client, model)\n",
    "            json_fin = json.loads(final_response)\n",
    "            write_json_to_txt(json_fin, scenario + str(i) + \".txt\")\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with scenario '{scenario}' iteration {i}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
