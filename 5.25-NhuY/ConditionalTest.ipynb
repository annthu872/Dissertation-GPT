{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def askJSON(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"String has been written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt\n",
    "MAIN_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow of a use case.\n",
    "Then you predict all scenarios that can happen in this flow.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- Stay close to the details described in the flow.\n",
    "- Focus on important and likely scenarios, important scenario is the scenario that users are more likely to encounter it. \n",
    "- Minimize the appearance of rare scenarios. \n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Test scenarios should be derived from cohesive sequences of steps that represent meaningful user interactions, rather than isolated steps.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "SUB_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow and one alternative or exception flow of a use case.\n",
    "Then you predict all scenarios that can lead user from the main flow to change to the alternative or exception flow mentioned for creating test cases.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Test scenarios should be derived from cohesive sequences of steps that represent meaningful user interactions, rather than isolated steps.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "- Do not generate scenarios with user analysis. (Example: User accidentally do A and user intentionally do A is the same scenario, so do not consider about \"accidentally\" or \"intentionally\" in scenario)\n",
    "- Do not choose another option that is not chosen by the flow, eventhough it is mentioned (Example: A pop up with OK and Cancel, the flow only has step choose OK. Do not generate scenario that press Cancel)\n",
    "- Do not generate scenario to test only the main flow.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "EXTRACT_CONDITION_SYSTEM_PROMPT=\"\"\"\n",
    "Given use case flows of a feature.\n",
    "Your task is to identify all the interactive elements within the feature. \n",
    "For each interactive element:\n",
    "Identify what type of that element (button,buttons, icon,scroll, text field,text area, tab, radio buttons, menu, combobox, sliders, switches, dialog, link, form,rating, filter).\n",
    "Identify all the conditions mentioned in the use case of that element that would make the element valid and the conditions that would make the element invalid based on the description of the use case flow.\n",
    "Do not arbitrarily create additional conditions that not mention in the use case flow.\n",
    "Return the element extracted in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\"Name of interactive element\": {\"condition\": {valid:\"conditions that make element valid\", invalid: \"conditions that make element invalid\"}, \"type\": \"element type\"}}\n",
    "Examples of output json format template: \n",
    "{\"Username\": {\"condition\": {\"valid\": \"must be over 8 characters and below 30 characters, must be entered\", racter, empty\"}, \"type\": \"text field\"}}\n",
    "{\"Search button\": {\"condition\": {\"valid\": \"\", \"invalid\": \"\"}, \"type\": \"text field\"}}\n",
    "\"\"\"\n",
    "\n",
    "GEN_SCENARIO_FOR_CONDITION_PROMPT=\"\"\"\n",
    "Given a list of interaction element for input value, their extracted conditions and corresponding use case.\n",
    "For each given invalid condition:\n",
    "- Generate a test scenario that test that condition.\n",
    "Do not generate test scenario to test element/condition that not mention in the given element list.\n",
    "Do not generate test scenario to test valid conditions.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "FILTER_SYSTEM_PROMPT=\"\"\"\n",
    "You will be provide with a use case and a list of test scenario.\n",
    "Based on information in the use case flow, define what test scenario is necessary to test the use case and remove duplicate test scenarios.\n",
    "Remove test case test system load error that not be mentioned in use case such as: Load Failure, System Error,Network Error,Non-Existent ...(and use case not mentioned these flow)\n",
    "Make sure test scenarios filted cover all the flow of use case and every scenarios in the response is unique.\n",
    "Return scenario name only.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Search for Books, Seminars, and Library Staff Members\"\n",
    "project_name = \"Library\"\n",
    "main_flow_prompt=\"\"\"\n",
    "functionName:\n",
    "    Search for Books, Seminars, and Library Staff Members\n",
    "objective:\n",
    "    Allow library users to search for books, seminars, and library staff members\n",
    "Descriptive sentences related to the function:\n",
    "    - The Library Management System will be PC-based with internet, allowing library users to search for books, seminars, and library staff members\n",
    "    - Library user is allowed to use the DLSSYSTEM only for searching book records\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "functionName:\n",
    "    Search for Books, Seminars, and Library Staff Members\n",
    "objective:\n",
    "    Allow library users to search for books, seminars, and library staff members\n",
    "Descriptive sentences related to the function:\n",
    "    - The Library Management System will be PC-based with internet, allowing library users to search for books, seminars, and library staff members\n",
    "    - Library user is allowed to use the DLSSYSTEM only for searching book records\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library\\Search for Books, Seminars, and Library Staff Members-1.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print(f\"{project_name}\\\\{usecase_name}-{i}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{'Search field': {'condition': {'valid': 'must be used for searching book records', 'invalid': 'used for purposes other than searching book records'}, 'type': 'text field'}}\n",
      "Scenario Name: Search for Non-Book Records\n",
      "\n",
      "functionName:\n",
      "    Search for Books, Seminars, and Library Staff Members\n",
      "objective:\n",
      "    Allow library users to search for books, seminars, and library staff members\n",
      "Descriptive sentences related to the function:\n",
      "    - The Library Management System will be PC-based with internet, allowing library users to search for books, seminars, and library staff members\n",
      "    - Library user is allowed to use the DLSSYSTEM only for searching book records\n",
      "\n",
      "\n",
      "main_gpt_response:1. Search for Books\n",
      "2. Search for Seminars\n",
      "3. Search for Library Staff Members\n",
      "\n",
      "filtercontent: \n",
      "Condition Scenario: Scenario Name: Search for Non-Book Records\n",
      " Flow cover scenarios: 1. Search for Books\n",
      "2. Search for Seminars\n",
      "3. Search for Library Staff Members\n",
      "filter_gpt_response \n",
      "1. Search for Books\n",
      "2. Search for Seminars\n",
      "3. Search for Library Staff Members\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'Library\\\\Search for Books, Seminars, and Library Staff Members-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Display Information of E-Book Being Downloaded\"\n",
    "project_name = \"Library\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    functionName:\n",
    "      Display Information of E-Book Being Downloaded\n",
    "    objective:\n",
    "      Display the information of the e-book being downloaded including ISBN, title, and location\n",
    "    Descriptive sentences related to the function:\n",
    "      - When downloading the books, the system shall display the information of the e-book which is just being downloaded including: ISBN, title, location\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    functionName:\n",
    "      Display Information of E-Book Being Downloaded\n",
    "    objective:\n",
    "      Display the information of the e-book being downloaded including ISBN, title, and location\n",
    "    Descriptive sentences related to the function:\n",
    "      - When downloading the books, the system shall display the information of the e-book which is just being downloaded including: ISBN, title, location\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{'ISBN': {'condition': {'valid': 'must be displayed during download', 'invalid': 'not displayed'}, 'type': 'text field'}, 'Title': {'condition': {'valid': 'must be displayed during download', 'invalid': 'not displayed'}, 'type': 'text field'}, 'Location': {'condition': {'valid': 'must be displayed during download', 'invalid': 'not displayed'}, 'type': 'text field'}}\n",
      "1. Scenario: ISBN Not Displayed During E-Book Download\n",
      "2. Scenario: Title Not Displayed During E-Book Download\n",
      "3. Scenario: Location Not Displayed During E-Book Download\n",
      "\n",
      "    functionName:\n",
      "      Display Information of E-Book Being Downloaded\n",
      "    objective:\n",
      "      Display the information of the e-book being downloaded including ISBN, title, and location\n",
      "    Descriptive sentences related to the function:\n",
      "      - When downloading the books, the system shall display the information of the e-book which is just being downloaded including: ISBN, title, location\n",
      "\n",
      "\n",
      "main_gpt_response:1. Successful Display of E-Book Information During Download\n",
      "\n",
      "filtercontent: \n",
      "Condition Scenario: 1. Scenario: ISBN Not Displayed During E-Book Download\n",
      "2. Scenario: Title Not Displayed During E-Book Download\n",
      "3. Scenario: Location Not Displayed During E-Book Download\n",
      " Flow cover scenarios: 1. Successful Display of E-Book Information During Download\n",
      "filter_gpt_response \n",
      "1. Scenario: ISBN Not Displayed During E-Book Download\n",
      "2. Scenario: Title Not Displayed During E-Book Download\n",
      "3. Scenario: Location Not Displayed During E-Book Download\n",
      "4. Successful Display of E-Book Information During Download\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'Library\\\\Display Information of E-Book Being Downloaded-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    If the temperature change is requested, then the determine heating/ cooling mode process is activated and makes a heating/ cooling request.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    If the temperature change is requested, then the determine heating/ cooling mode process is activated and makes a heating/ cooling request.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{}\n",
      "\n",
      "    If the temperature change is requested, then the determine heating/ cooling mode process is activated and makes a heating/ cooling request.\n",
      "\n",
      "\n",
      "main_gpt_response:1. Scenario: Temperature Increase Request Triggers Heating Mode\n",
      "2. Scenario: Temperature Decrease Request Triggers Cooling Mode\n",
      "\n",
      "filtercontent: \n",
      " Flow cover scenarios: 1. Scenario: Temperature Increase Request Triggers Heating Mode\n",
      "2. Scenario: Temperature Decrease Request Triggers Cooling Mode\n",
      "filter_gpt_response \n",
      "1. Scenario: Temperature Increase Request Triggers Heating Mode\n",
      "2. Scenario: Temperature Decrease Request Triggers Cooling Mode\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    If the current temperature is strictly less than the lower value of the valid temperature range or if if the received temperature value is strictly greater than the upper value of the valid temperture range, then the THEMAS system shall identify the current temperature value as invalid temperature and shall output an invalid temperature status.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    If the current temperature is strictly less than the lower value of the valid temperature range or if if the received temperature value is strictly greater than the upper value of the valid temperture range, then the THEMAS system shall identify the current temperature value as invalid temperature and shall output an invalid temperature status.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{'Temperature value': {'condition': {'valid': 'is within the valid temperature range', 'invalid': 'is strictly less than the lower value of the valid temperature range or is strictly greater than the upper value of the valid temperature range'}, 'type': 'text field'}}\n",
      "Scenario 1: Temperature Value Below Lower Limit Test  \n",
      "Scenario 2: Temperature Value Above Upper Limit Test\n",
      "\n",
      "    If the current temperature is strictly less than the lower value of the valid temperature range or if if the received temperature value is strictly greater than the upper value of the valid temperture range, then the THEMAS system shall identify the current temperature value as invalid temperature and shall output an invalid temperature status.\n",
      "\n",
      "\n",
      "main_gpt_response:1. Temperature Below Valid Range Scenario\n",
      "2. Temperature Above Valid Range Scenario\n",
      "3. Temperature Within Valid Range Scenario\n",
      "\n",
      "filtercontent: \n",
      "Condition Scenario: Scenario 1: Temperature Value Below Lower Limit Test  \n",
      "Scenario 2: Temperature Value Above Upper Limit Test\n",
      " Flow cover scenarios: 1. Temperature Below Valid Range Scenario\n",
      "2. Temperature Above Valid Range Scenario\n",
      "3. Temperature Within Valid Range Scenario\n",
      "filter_gpt_response \n",
      "Scenario 1: Temperature Value Below Lower Limit Test\n",
      "Scenario 2: Temperature Value Above Upper Limit Test\n",
      "Scenario 3: Temperature Within Valid Range Scenario\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    Temperature values that do not exceed these limits shall be output for subsequent processing.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    Temperature values that do not exceed these limits shall be output for subsequent processing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{}\n",
      "\n",
      "    Temperature values that do not exceed these limits shall be output for subsequent processing.\n",
      "\n",
      "\n",
      "main_gpt_response:Given the information and the rules provided, this flow describes a process where temperature values are checked against certain limits, and only those that do not exceed these limits are passed on for further processing. Based on this, here is the scenario:\n",
      "\n",
      "1. Temperature values within acceptable limits are processed successfully.\n",
      "\n",
      "filtercontent: \n",
      " Flow cover scenarios: Given the information and the rules provided, this flow describes a process where temperature values are checked against certain limits, and only those that do not exceed these limits are passed on for further processing. Based on this, here is the scenario:\n",
      "\n",
      "1. Temperature values within acceptable limits are processed successfully.\n",
      "filter_gpt_response \n",
      "1. Temperature values within acceptable limits are processed successfully.\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{}\n",
      "\n",
      "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
      "\n",
      "\n",
      "main_gpt_response:Based on the provided information, the main flow of the use case involves a condition check that leads to an action (turning on the heating unit) if a specific condition (LO = TLT) is met. Given the rules for predicting scenarios and focusing on important and likely scenarios while minimizing rare ones, here are the scenarios:\n",
      "\n",
      "1. Heating Unit Activation Scenario: This scenario covers the condition where LO equals TLT, leading to the activation of the heating unit as described in the flow.\n",
      "\n",
      "2. No Action Scenario: This scenario occurs when the condition LO = TLT is not met, resulting in no request to turn on the heating unit.\n",
      "\n",
      "filtercontent: \n",
      " Flow cover scenarios: Based on the provided information, the main flow of the use case involves a condition check that leads to an action (turning on the heating unit) if a specific condition (LO = TLT) is met. Given the rules for predicting scenarios and focusing on important and likely scenarios while minimizing rare ones, here are the scenarios:\n",
      "\n",
      "1. Heating Unit Activation Scenario: This scenario covers the condition where LO equals TLT, leading to the activation of the heating unit as described in the flow.\n",
      "\n",
      "2. No Action Scenario: This scenario occurs when the condition LO = TLT is not met, resulting in no request to turn on the heating unit.\n",
      "filter_gpt_response \n",
      "Heating Unit Activation Scenario\n",
      "No Action Scenario\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{}\n",
      "\n",
      "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
      "\n",
      "\n",
      "main_gpt_response:Based on the provided information, this flow describes a specific behavior of a heating/cooling unit in relation to the THEMAS system, focusing on the real-time update of statuses without delay. Given the constraints and guidance for predicting scenarios, it appears that this flow inherently suggests only one main scenario due to the lack of variability in user actions or conditions that could alter the outcome. Therefore, the scenario predicted is:\n",
      "\n",
      "1. Real-time Status Update from Heating/Cooling Unit to THEMAS System\n",
      "\n",
      "filtercontent: \n",
      " Flow cover scenarios: Based on the provided information, this flow describes a specific behavior of a heating/cooling unit in relation to the THEMAS system, focusing on the real-time update of statuses without delay. Given the constraints and guidance for predicting scenarios, it appears that this flow inherently suggests only one main scenario due to the lack of variability in user actions or conditions that could alter the outcome. Therefore, the scenario predicted is:\n",
      "\n",
      "1. Real-time Status Update from Heating/Cooling Unit to THEMAS System\n",
      "filter_gpt_response \n",
      "Real-time Status Update from Heating/Cooling Unit to THEMAS System\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{'Unique identifier': {'condition': {'valid': 'must be unique within the THEMAS system', 'invalid': 'is not unique within the THEMAS system'}, 'type': 'text field'}}\n",
      "Scenario Name: Verify Thermostat Registration with Duplicate Unique Identifier\n",
      "\n",
      "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
      "\n",
      "\n",
      "main_gpt_response:1. Successful Thermostat Registration in THEMAS System\n",
      "\n",
      "filtercontent: \n",
      "Condition Scenario: Scenario Name: Verify Thermostat Registration with Duplicate Unique Identifier\n",
      " Flow cover scenarios: 1. Successful Thermostat Registration in THEMAS System\n",
      "filter_gpt_response \n",
      "Verify Thermostat Registration with Duplicate Unique Identifier\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{}\n",
      "\n",
      "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
      "\n",
      "\n",
      "main_gpt_response:1. Event Successfully Identified and Message Formatted Correctly\n",
      "2. Event Type Not Recognized\n",
      "3. Event Message Formatting Error\n",
      "\n",
      "filtercontent: \n",
      " Flow cover scenarios: 1. Event Successfully Identified and Message Formatted Correctly\n",
      "2. Event Type Not Recognized\n",
      "3. Event Message Formatting Error\n",
      "filter_gpt_response \n",
      "1. Event Successfully Identified and Message Formatted Correctly\n",
      "2. Event Type Not Recognized\n",
      "3. Event Message Formatting Error\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"Temperature change\"\n",
    "project_name = \"THEMAS\"\n",
    "main_flow_prompt=\"\"\"\n",
    "    If the temperature change is requested, then the determine heating/cooling mode process is activated and makes a heating/cooling request.\n",
    "    If the current temperature value is strictly less than the lower value of the valid temperature range  or if the received temperature value is strictly greater than the upper value of the valid temperature range, then the THEMAS system shall identify the current temperature value as an invalid temperature and shall output an invalid temperature status.\n",
    "    The THEMAS system shall maintain the ON/OFF status of each heating and cooling unit.\n",
    "    Temperatures that do not exceed these limits shall be output for subsequent processing.\n",
    "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
    "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
    "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
    "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "    If the temperature change is requested, then the determine heating/cooling mode process is activated and makes a heating/cooling request.\n",
    "    If the current temperature value is strictly less than the lower value of the valid temperature range  or if the received temperature value is strictly greater than the upper value of the valid temperature range, then the THEMAS system shall identify the current temperature value as an invalid temperature and shall output an invalid temperature status.\n",
    "    The THEMAS system shall maintain the ON/OFF status of each heating and cooling unit.\n",
    "    Temperatures that do not exceed these limits shall be output for subsequent processing.\n",
    "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
    "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
    "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
    "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 1\n",
      "{'Current temperature value': {'condition': {'valid': 'strictly less than the lower value of the valid temperature range or strictly greater than the upper value of the valid temperature range', 'invalid': 'within the valid temperature range'}, 'type': 'text field'}}\n",
      "Scenario 1: \"Submit Temperature Within Valid Range\"\n",
      "\n",
      "    If the temperature change is requested, then the determine heating/cooling mode process is activated and makes a heating/cooling request.\n",
      "    If the current temperature value is strictly less than the lower value of the valid temperature range  or if the received temperature value is strictly greater than the upper value of the valid temperature range, then the THEMAS system shall identify the current temperature value as an invalid temperature and shall output an invalid temperature status.\n",
      "    The THEMAS system shall maintain the ON/OFF status of each heating and cooling unit.\n",
      "    Temperatures that do not exceed these limits shall be output for subsequent processing.\n",
      "    If this condition is true, then this module shall output a request to turn on the heating unit in case LO = TLT.\n",
      "    The heating/cooling unit shall have no real time delay when these statuses are sent to the THEMAS system.\n",
      "    Each thermostat shall have a unique identifier by which that thermostat is identified in the THEMAS system.\n",
      "    When an event occurs, the THEMAS system shall identify the event type and format an appropriate event message.\n",
      "\n",
      "\n",
      "main_gpt_response:1. Temperature Change Request Within Valid Range\n",
      "2. Temperature Change Request Below Lower Valid Range\n",
      "3. Temperature Change Request Above Upper Valid Range\n",
      "4. Heating Unit Activation Request\n",
      "5. Cooling Unit Activation Request\n",
      "6. Invalid Temperature Status Output\n",
      "7. Event Identification and Message Formatting\n",
      "\n",
      "filtercontent: \n",
      "Condition Scenario: Scenario 1: \"Submit Temperature Within Valid Range\"\n",
      " Flow cover scenarios: 1. Temperature Change Request Within Valid Range\n",
      "2. Temperature Change Request Below Lower Valid Range\n",
      "3. Temperature Change Request Above Upper Valid Range\n",
      "4. Heating Unit Activation Request\n",
      "5. Cooling Unit Activation Request\n",
      "6. Invalid Temperature Status Output\n",
      "7. Event Identification and Message Formatting\n",
      "filter_gpt_response \n",
      "Scenario 1: \"Submit Temperature Within Valid Range\"\n",
      "Scenario 2: \"Temperature Change Request Below Lower Valid Range\"\n",
      "Scenario 3: \"Temperature Change Request Above Upper Valid Range\"\n",
      "Scenario 4: \"Heating Unit Activation Request\"\n",
      "Scenario 5: \"Cooling Unit Activation Request\"\n",
      "Scenario 6: \"Invalid Temperature Status Output\"\n",
      "Scenario 7: \"Event Identification and Message Formatting\"\n",
      "An error occurred while writing to the file: [Errno 2] No such file or directory: 'THEMAS\\\\Temperature change-1.txt'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,2):\n",
    "  print(\"TIME \"+str(i))\n",
    "  filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "  promptExtractCondition = [\n",
    "      { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "  gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "  full_elements = json.loads(gpt_response)\n",
    "  condition_element = {key: value for key, value in full_elements.items() \n",
    "                                if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "  print(condition_element)\n",
    "  filtercontent = \"\"\n",
    "  if(len(condition_element)!= 0):\n",
    "    promptScenarioForCondition = [\n",
    "        { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "      ]\n",
    "    condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "    filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "    print(condition_scenario_response)\n",
    "  promptMainScenario = [\n",
    "      { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "    ]\n",
    "  main_gpt_response = ask(promptMainScenario, client, model)\n",
    "  filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "  print(main_flow_prompt)\n",
    "  print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "\n",
    "\n",
    "  print(\"filtercontent: \"+ filtercontent)\n",
    "  filtercontent1 += filtercontent\n",
    "  promptFilter = [\n",
    "      { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": filtercontent1}\n",
    "    ]\n",
    "  filter_gpt_response = ask(promptFilter, client, model)\n",
    "  print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "  write_string_to_file(f\"{project_name}\\\\{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
