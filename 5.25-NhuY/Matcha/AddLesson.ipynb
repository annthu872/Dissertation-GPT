{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt_raw(json_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in json_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt(json_data, file_name):\n",
    "    filtered_data = {k: v for k, v in json_data.items() if v.get(\"reflect\") == \"yes\"}\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in filtered_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_8=\"\"\"\n",
    "I want you to act as a software tester.\n",
    "Your task is to read the test scenario's name and the corresponding use case specification to base on those information for generateing test steps for test cases and their following expected result.\n",
    "Return the test cases in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"explanation\": \"Why do you create this test case? How does this test case related to the test scenario inputed?\",\n",
    "  ],\n",
    "}\n",
    "If there are more than one test case for this scenario, continue writing other test case in this form.\n",
    "\n",
    "Rules for generating test steps:\n",
    "- Describe the test step clearly to make sure each test case is independent, tester do not need to read other information (example: other test case, use case specification) to know how to do that step.\n",
    "- Avoid references to other test cases or instructions like \"do as mentioned.\"\n",
    "- If the test case need to be repeated to test with different order, data or case, seperate them to be distinct test cases.\n",
    "- If the scenario is about testing the displation and there is no flow directly cover that scenario, use only the basic (or main) flow to test it.\n",
    "- If there are use cases mentioned in extended or included use case, create test case combine use cases, Try to find the connection point of use cases for combination. \n",
    "- For test scenarios mentioning navigation in the name, only produce test cases related to the specified navigation method. \n",
    "(Example: \"Scenario: User navigates to a page by navbar\", only produce a test case of user navigates to that page by navbar even though the use case description has many way to navigate to that page)\n",
    "- For test scenarios not mentioning navigation in the name, do not include any navigation test cases.\n",
    "- Generate test cases that directly match the scenario name. Choose only one flow to cover the scenario.\n",
    "- Ensure all actions and objectives match the scenario name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_REF=\"\"\"\n",
    "Given a test scenario and test cases to test that given test scenario.\n",
    "Mark if test case can test the given test scenario or not through test steps, expected output, objective (although if it test other use case path or flow, if it is not used to test the given test scenario.)\n",
    "And give explanation why you think the resulted test case reflect the given test scenario or not. \n",
    "Your response should keep the format of the inserted test cases.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"reflect\": \"yes/no\",\n",
    "    \"explanation\": \"explain why you think this test case reflect the given test scenario or not\",\n",
    "  ],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"Successful Lesson Creation\", \"Lesson Creation with Missing Name\", \"Lesson Creation with Missing Description\",\n",
    "             \"Lesson Creation with Duplicate Name\", \"Navigate to Add New Lesson Page via Navbar Option\", \"Cancel Adding New Lesson Before Entering Name\",\n",
    "             \"Cancel Adding New Lesson After Entering Name\", \"Cancel Adding New Lesson After Entering Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uc = \"\"\"\n",
    "use case id: U16\n",
    "use case name: adding new lesson\n",
    "objective: This use case allows administrator to add a new english learning lesson.\n",
    "actor: administrator\n",
    "\n",
    "basic flow:\n",
    "Step 1: Admin presses the tab \"Lesson Management\" on the navbar.\n",
    "Step 2: The system open dropdown menu for Lesson with options: \"Add new Lesson\", \"View list of Lesson\", \"Lesson statistics\".\n",
    "Step 3: Administrator selects \"View list of Lesson\" options.\n",
    "Step 4: The system redirects Admin to the Lesson Management page which show lesson list which admins created.\n",
    "Step 5: Administrator select button \"Add new Lesson\"\n",
    "Step 6: The system redirects Admin to the \"Add new Lesson\" page.\n",
    "Step 7: Administrator adds the name of the lesson in the name field.\n",
    "Step 8: Administrator adds the description of the lesson in the description field.\n",
    "Step 9: Administrator selects \"Save\" to create a new lesson.\n",
    "\n",
    "Alternative flow 1: Administrator adds an already used name for the lesson.\n",
    "At step 3 of the basic flow: Administrator adds a name that is already used in the existed lessons in the name field.\n",
    "step 4: Administrator adds the description of the lesson in the description field.\n",
    "step 5: Administrator selects \"Save\" to create a new lesson.\n",
    "step 6: The system shows notification that the name is already used, prompting Administrator to add another name in the name field.\n",
    "step 7: Administrator adds a new name in the name field.\n",
    "step 8: Administrator selects \"Save\" to create a new lesson.\n",
    "\n",
    "Alternative flow 2: Navigate to Add new lesson page by options in navbar. \n",
    "At step 3 of the basic flow: Administrator selects \"Add new Lesson\".\n",
    "Go back to step 6 in the basic flow and continue with the steps from step 6.\n",
    "\n",
    "Exception flow:\n",
    "At step 3 of the basic flow: Administrator can select \"Cancel\" to stop adding a new lesson at anytime.\n",
    "\n",
    "Preconditions:\n",
    "Administrator must be logged into the application.\n",
    "Administrator is allowed to add new lesson. \n",
    "\n",
    "Postconditions:\n",
    "After successfully adding new lesson, the system will redirect Administrator to the adding topic to new lesson section.\n",
    "New lesson will appear in the lesson list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios:\n",
    "    i = 1\n",
    "    while(i<=3):\n",
    "        try: \n",
    "            promptTestCase = [\n",
    "            { \"role\": \"system\", \"content\": SYSTEM_PROMPT_8},\n",
    "            { \"role\": \"user\", \"content\": scenario + \"\\n\" + prompt_uc}\n",
    "            ]\n",
    "            gpt_response = ask(promptTestCase, client, model)\n",
    "            json_data = json.loads(gpt_response)\n",
    "            write_json_to_txt_raw(json_data, scenario + \"-Raw\" + str(i) + \".txt\")\n",
    "\n",
    "            promptSelfRef = [\n",
    "                { \"role\": \"system\", \"content\": SELF_REF},\n",
    "                { \"role\": \"user\", \"content\": scenario + \"\\n\" + gpt_response}\n",
    "            ]\n",
    "            final_response = ask(promptSelfRef, client, model)\n",
    "            json_fin = json.loads(final_response)\n",
    "            write_json_to_txt(json_fin, scenario + str(i) + \".txt\")\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with scenario '{scenario}' iteration {i}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
