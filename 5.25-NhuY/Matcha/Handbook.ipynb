{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt_raw(json_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in json_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt(json_data, file_name):\n",
    "    filtered_data = {k: v for k, v in json_data.items() if v.get(\"reflect\") == \"yes\"}\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in filtered_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_8=\"\"\"\n",
    "I want you to act as a software tester.\n",
    "Your task is to read the test scenario's name and the corresponding use case specification to base on those information for generateing test steps for test cases and their following expected result.\n",
    "Return the test cases in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"explanation\": \"Why do you create this test case? How does this test case related to the test scenario inputed?\",\n",
    "  ],\n",
    "}\n",
    "If there are more than one test case for this scenario, continue writing other test case in this form.\n",
    "\n",
    "Rules for generating test steps:\n",
    "- Describe the test step clearly to make sure each test case is independent, tester do not need to read other information (example: other test case, use case specification) to know how to do that step.\n",
    "- Avoid references to other test cases or instructions like \"do as mentioned.\"\n",
    "- If the test case need to be repeated to test with different order, data or case, seperate them to be distinct test cases.\n",
    "- If the scenario is about testing the displation and there is no flow directly cover that scenario, use only the basic (or main) flow to test it.\n",
    "- If there are use cases mentioned in extended or included use case, create test case combine use cases, Try to find the connection point of use cases for combination. \n",
    "- For test scenarios mentioning navigation in the name, only produce test cases related to the specified navigation method. \n",
    "(Example: \"Scenario: User navigates to a page by navbar\", only produce a test case of user navigates to that page by navbar even though the use case description has many way to navigate to that page)\n",
    "- For test scenarios not mentioning navigation in the name, do not include any navigation test cases.\n",
    "- Generate test cases that directly match the scenario name. Choose only one flow to cover the scenario.\n",
    "- Ensure all actions and objectives match the scenario name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_REF=\"\"\"\n",
    "Given a test scenario and test cases to test that given test scenario.\n",
    "Mark if test case can test the given test scenario or not through test steps, expected output, objective (although if it test other use case path or flow, if it is not used to test the given test scenario.)\n",
    "And give explanation why you think the resulted test case reflect the given test scenario or not. \n",
    "Your response should keep the format of the inserted test cases.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"reflect\": \"yes/no\",\n",
    "    \"explanation\": \"explain why you think this test case reflect the given test scenario or not\",\n",
    "  ],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"Successful Display of Vocabulary List\", \"Scroll Through Vocabulary List\",\n",
    "             \"Learner uses the search function to find a specific vocabulary word in the Handbook\",\n",
    "             \"Search for a Meaningless Vocabulary in Handbook\",\n",
    "             \"Learner Excludes Vocabulary from Review Test\",\n",
    "             \"Marking Vocabulary for Review in Daily Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uc = \"\"\"\n",
    "use case id: \n",
    "use case name: handbook\n",
    "objective: This use case allows to see the vocabulary that learner has learned and setup the vocabulary review test.\n",
    "actor: learner \n",
    "Preconditions:\n",
    "Learner must be logged into the application.\n",
    "\n",
    "basic flow:\n",
    "Step 1: Learner selects the \"Handbook\" tab on the navigation bar.\n",
    "Step 2:The system displays a list of leaner's vocabulary list. The list of vocabulary is displayed in vertical rows, each word is a horizontal row displaying the word name, word type, word meaning, memorization level and has a checkbox to mark the appearance of the vocabulary in the review test.\n",
    "Step 3:Learners can scroll the scroll bar to see more vocabulary.\n",
    "\n",
    "Alternative Flows:\n",
    "Alternative flow 1: Learner search vocabulary in Handbook.\n",
    "At step 3 of the basic flow: Learner presses the search bar.\n",
    "Step 4: Learner enters the word to search.\n",
    "Step 5: The system filters vocabulary containing characters entered by learners in the search bar.\n",
    "\n",
    "Alternative flow 2:Learner search a meaningless vocabulary in Handbook.\n",
    "At step 3 of the basic flow: Learner presses the search bar.\n",
    "Step 4: Learner enters a meaningless word to search.\n",
    "Step 5: The system notifies that there are no suitable search results.\n",
    "\n",
    "Alternative flow 3: Learner set up 1 word to not be review in the review daily test\n",
    "At step 3 of the basic flow: Learner untick a checkbox of a vocabulary.\n",
    "Step 4: the system mark that vocabulary will not appear in the daily test review.\n",
    "\n",
    "Alternative flow 4: Learner set up 1 word to be review in the review daily test\n",
    "At step 3 of the basic flow: Learner tick a checkbox of a vocabulary.\n",
    "Step 4: the system mark that vocabulary will appear in the daily test review.\n",
    "\n",
    "Postcondition: \n",
    "None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in scenarios:\n",
    "    i = 1\n",
    "    while(i<=1):\n",
    "        try: \n",
    "            promptTestCase = [\n",
    "            { \"role\": \"system\", \"content\": SYSTEM_PROMPT_8},\n",
    "            { \"role\": \"user\", \"content\": scenario + \"\\n\" + prompt_uc}\n",
    "            ]\n",
    "            gpt_response = ask(promptTestCase, client, model)\n",
    "            json_data = json.loads(gpt_response)\n",
    "            write_json_to_txt_raw(json_data, scenario + \"-Raw\" + str(i) + \".txt\")\n",
    "\n",
    "            promptSelfRef = [\n",
    "                { \"role\": \"system\", \"content\": SELF_REF},\n",
    "                { \"role\": \"user\", \"content\": scenario + \"\\n\" + gpt_response}\n",
    "            ]\n",
    "            final_response = ask(promptSelfRef, client, model)\n",
    "            json_fin = json.loads(final_response)\n",
    "            write_json_to_txt(json_fin, scenario + str(i) + \".txt\")\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with scenario '{scenario}' iteration {i}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
