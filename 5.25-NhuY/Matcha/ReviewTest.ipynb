{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt_raw(json_data, file_name):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in json_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_txt(json_data, file_name):\n",
    "    filtered_data = {k: v for k, v in json_data.items() if v.get(\"reflect\") == \"yes\"}\n",
    "    with open(file_name, 'w') as file:\n",
    "        for test_case, details in filtered_data.items():\n",
    "            file.write(f\"Test Case: {test_case}\\n\")\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, list):\n",
    "                    file.write(f\"{key}:\\n\")\n",
    "                    for item in value:\n",
    "                        file.write(f\"  - {item}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"{key}: {value}\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_8=\"\"\"\n",
    "I want you to act as a software tester.\n",
    "Your task is to read the test scenario's name and the corresponding use case specification to base on those information for generateing test steps for test cases and their following expected result.\n",
    "Return the test cases in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"explanation\": \"Why do you create this test case? How does this test case related to the test scenario inputed?\",\n",
    "  ],\n",
    "}\n",
    "If there are more than one test case for this scenario, continue writing other test case in this form.\n",
    "\n",
    "Rules for generating test steps:\n",
    "- Describe the test step clearly to make sure each test case is independent, tester do not need to read other information (example: other test case, use case specification) to know how to do that step.\n",
    "- Avoid references to other test cases or instructions like \"do as mentioned.\"\n",
    "- If the test case need to be repeated to test with different order, data or case, seperate them to be distinct test cases.\n",
    "- If the scenario is about testing the displation and there is no flow directly cover that scenario, use only the basic (or main) flow to test it.\n",
    "- If there are use cases mentioned in extended or included use case, create test case combine use cases, Try to find the connection point of use cases for combination. \n",
    "- For test scenarios mentioning navigation in the name, only produce test cases related to the specified navigation method. \n",
    "(Example: \"Scenario: User navigates to a page by navbar\", only produce a test case of user navigates to that page by navbar even though the use case description has many way to navigate to that page)\n",
    "- For test scenarios not mentioning navigation in the name, do not include any navigation test cases.\n",
    "- Generate test cases that directly match the scenario name. Choose only one flow to cover the scenario.\n",
    "- Ensure all actions and objectives match the scenario name.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_REF=\"\"\"\n",
    "Given a test scenario and test cases to test that given test scenario.\n",
    "Mark if test case can test the given test scenario or not through test steps, expected output, objective (although if it test other use case path or flow, if it is not used to test the given test scenario.)\n",
    "And give explanation why you think the resulted test case reflect the given test scenario or not. \n",
    "Your response should keep the format of the inserted test cases.\n",
    "The JSON format should follow the following structure:\n",
    "{\n",
    "  \"Test Case 1\":[\n",
    "    \"testCaseName\": \"Clear name of the test case so tester know what to test when they first read\",\n",
    "    \"objective\": \"Verify who doing what action or function in the test case and the summary of the final result of the test case\",\n",
    "    \"testSteps\": [\n",
    "      \"Step 1: Describe the step.\",\n",
    "      \"Step 2: Describe the step.\",\n",
    "      \"Step 3: Describe the step.\"\n",
    "    ],\n",
    "    \"expectedResult\": \"You inform the tester what should they see after doing all the steps\",\n",
    "    \"reflect\": \"yes/no\",\n",
    "    \"explanation\": \"explain why you think this test case reflect the given test scenario or not\",\n",
    "  ],\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\"Review with Question Method\", \"Review with Sound Method\", \" Review with Image Method\",\n",
    "             \"Review with Meaning Method\", \"Learners choose the wrong answer\", \"Completing the Last Review Word\",\n",
    "             \"Exit and Choose to Stay Scenario\", \"Scenario Learner decides to exit during review session before selecting an answer.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_uc = \"\"\"\n",
    "Review test\n",
    "Main flow:\n",
    "Step 1: Learners select the review button on the Home page. \n",
    "Step 2: The system redirects learners to the review's screen.\n",
    "Step 3: System randomly selects one of four methods including questions, sound, image or meanings of one word for each words in learner's daily review words list with four selections of answer in the bottom of the screen. \n",
    "Step 4: Learners chooses by clicking on one of four selections.\n",
    "Step 5: Learners click the right answer. \n",
    "Step 6: System shows a green word for learners to read. \n",
    "Step 7: This word is increased by the system in memory level.\n",
    "Step 8: Learners press \"Continue\" button to go to the next word in the review words list.\n",
    "\n",
    "Alternative flow 1: Learners choose the wrong answer\n",
    "At step 5 of the basic flow: Learners click the wrong answer. \n",
    "Step 6: System shows a red word for learners to read. . \n",
    "Step 7: This word is decreased by the system in memory level.\n",
    "Go back to step 8 in the basic flow and continue with the steps from step 8.\n",
    "\n",
    "Alternative flow 2: Learners have done the review by coming to the last word in the review list\n",
    "At step 8 of the basic flow: Learners press \"Continue\" button to go back to the Home page.\n",
    "Step 9: System saved the review.\n",
    "\n",
    "Alternative flow 3: Learners press \"Exit\" button and choose to stay\n",
    "At step 4 of the basic flow: Learners press \"Exit\" button.\n",
    "Step 5: System displays a pop up with two buttons \"Confirm\" and \"Stay\". \n",
    "Step 6: Learners choose \"Stay\".\n",
    "Go back to step 4 in the basic flow and continue with the steps from step 4\n",
    "\n",
    "Exception flow 1: Learners press \"Exit\" button and choose to exit\n",
    "At step 3 of the basic flow: Learners press \"Exit\" button.\n",
    "Step 4: System displays a pop up with two buttons \"Confirm\" and \"Stay\". \n",
    "Step 5: Learners choose \"Confirm\". \n",
    "Step 6: System redirect learners to Home page and the review will not be saved.\n",
    "\n",
    "Pre condition:\n",
    "User has login as learner. In their account, there are some words has been learned so there will be a review words list for today.\n",
    "\n",
    "Post condition:\n",
    "Update the status of vocabulary (memory level, review time) corresponding to the results of exercise checking. The status of the vocabulary to be reviewed is changed. The done review is saved and user could look back.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred with scenario 'Exit and Choose to Stay Scenario' iteration 1: 'list' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "for scenario in scenarios:\n",
    "    i = 1\n",
    "    while(i<=1):\n",
    "        try: \n",
    "            promptTestCase = [\n",
    "            { \"role\": \"system\", \"content\": SYSTEM_PROMPT_8},\n",
    "            { \"role\": \"user\", \"content\": scenario + \"\\n\" + prompt_uc}\n",
    "            ]\n",
    "            gpt_response = ask(promptTestCase, client, model)\n",
    "            json_data = json.loads(gpt_response)\n",
    "            write_json_to_txt_raw(json_data, scenario + \"-Raw\" + str(i) + \".txt\")\n",
    "\n",
    "            promptSelfRef = [\n",
    "                { \"role\": \"system\", \"content\": SELF_REF},\n",
    "                { \"role\": \"user\", \"content\": scenario + \"\\n\" + gpt_response}\n",
    "            ]\n",
    "            final_response = ask(promptSelfRef, client, model)\n",
    "            json_fin = json.loads(final_response)\n",
    "            write_json_to_txt(json_fin, scenario + str(i) + \".txt\")\n",
    "            i+=1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with scenario '{scenario}' iteration {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTestCase = [\n",
    "{ \"role\": \"system\", \"content\": SYSTEM_PROMPT_8},\n",
    "{ \"role\": \"user\", \"content\": scenarios[7] + \"\\n\" + prompt_uc}\n",
    "]\n",
    "gpt_response = ask(promptTestCase, client, model)\n",
    "json_data = json.loads(gpt_response)\n",
    "write_json_to_txt_raw(json_data, scenarios[7] + \"-Raw\" + \".txt\")\n",
    "\n",
    "promptSelfRef = [\n",
    "    { \"role\": \"system\", \"content\": SELF_REF},\n",
    "    { \"role\": \"user\", \"content\": scenarios[7] + \"\\n\" + gpt_response}\n",
    "]\n",
    "final_response = ask(promptSelfRef, client, model)\n",
    "json_fin = json.loads(final_response)\n",
    "write_json_to_txt(json_fin, scenarios[7] + \".txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
