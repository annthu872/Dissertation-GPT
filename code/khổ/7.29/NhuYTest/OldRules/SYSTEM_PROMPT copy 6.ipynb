{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def askJSON(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    # Detect the encoding\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "    \n",
    "    # Read the file with the detected encoding\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file at path {file_path} was not found.\"\n",
    "    except UnicodeDecodeError:\n",
    "        return f\"Error: The file at path {file_path} cannot be decoded with the {encoding} encoding.\"\n",
    "    except IOError:\n",
    "        return f\"Error: An I/O error occurred while reading the file at path {file_path}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"String has been written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about a use case.\n",
    "Then you predict all scenarios that can happen in this use case.\n",
    "\n",
    "Rules of generating scenarios you should follow:\n",
    "- Scenarios involve executing all steps within a specific flow with varied data or actions.\n",
    "- Scenarios should be independent and require a complete flow to proceed.\n",
    "- A scenario encompasses a whole function, not just verifying individual steps.\n",
    "- Each flow must have at least one test scenario to cover the flow.\n",
    "- Ensure that the test scenarios cover all the necessary aspects of the use case flow.\n",
    "\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about a use case.\n",
    "Then you predict all scenarios that can happen in this use case.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- Stay close to the details described in the flow.\n",
    "- Focus on important and likely scenarios, important scenario is the scenario that users are more likely to encounter it. \n",
    "- Minimize the appearance of rare scenarios. \n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Do not generate scenarios with user analysis. (Example: User accidentally do A and user intentionally do A is the same scenario, so do not consider about \"accidentally\" or \"intentionally\" in scenario)\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- Do not arbitrarily create additional test scenarios to test cases that not mention in the use case flow.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_path = r\"C:\\Users\\congc\\Desktop\\GPT\\Dissertation-GPT\\dataset\\SpecificationData\\Adventura\"\n",
    "# save_path = r\"D:\\Dissertation-GPT\\evaluate\\prove\\Matcha\\ts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name_list = []\n",
    "project_name = os.path.basename(usecase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adventura - Filter Event', 'Adventura - Filter Attraction', 'Adventura - Filter Event', 'Adventura - Filter', 'Adventura - view attraction', 'Adventura - view event information', 'Adventura- Search Attraction', 'Adventura- Search Event', 'Save Favorite']\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(usecase_path):\n",
    "    usecase_name_list.append (filename.split(\".txt\")[0])\n",
    "print(usecase_name_list)\n",
    "usecase_name_list=[\"Adventura - Filter Attraction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Attraction\n",
      "gpt_response \n",
      "1. Filter search results successfully\n",
      "2. Filter search results with no suitable result\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Attraction\n",
      "gpt_response \n",
      "1. Filter search results successfully\n",
      "2. Filter search results with no suitable result\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Attraction\n",
      "gpt_response \n",
      "1. Filter search results successfully\n",
      "2. Filter search results with no suitable result\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. Filter events by selecting \"All\" status\n",
      "2. Filter events by selecting \"Happening\" status\n",
      "3. Filter events by selecting \"In coming\" status\n",
      "4. Filter events by selecting \"Closed\" status\n",
      "5. No suitable results for selected status\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. Filter events by \"All\" status\n",
      "2. Filter events by \"Happening\" status\n",
      "3. Filter events by \"In coming\" status\n",
      "4. Filter events by \"Closed\" status\n",
      "5. No suitable results for selected status\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. Filter events by selecting \"All\" status\n",
      "2. Filter events by selecting \"Happening\" status\n",
      "3. Filter events by selecting \"In coming\" status\n",
      "4. Filter events by selecting \"Closed\" status\n",
      "5. No suitable results for selected status\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. FilterEvents_AllStatus - User filters events with the status 'All'.\n",
      "2. FilterEvents_HappeningStatus - User filters events with the status 'Happening'.\n",
      "3. FilterEvents_IncomingStatus - User filters events with the status 'Incoming'.\n",
      "4. FilterEvents_ClosedStatus - User filters events with the status 'Closed'.\n",
      "5. FilterEvents_NoSuitableResult - User selects a status but the system has no suitable results.\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. FilterEvents_AllStatus\n",
      "2. FilterEvents_HappeningStatus\n",
      "3. FilterEvents_IncomingStatus\n",
      "4. FilterEvents_ClosedStatus\n",
      "5. FilterEvents_NoSuitableResult\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventura - Filter Event\n",
      "gpt_response \n",
      "1. Filter Events by Happening Status\n",
      "2. Filter Events by Incoming Status\n",
      "3. Filter Events by Closed Status\n",
      "4. Filter Events with No Suitable Results\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
