{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def askJSON(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    # Detect the encoding\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "    \n",
    "    # Read the file with the detected encoding\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file at path {file_path} was not found.\"\n",
    "    except UnicodeDecodeError:\n",
    "        return f\"Error: The file at path {file_path} cannot be decoded with the {encoding} encoding.\"\n",
    "    except IOError:\n",
    "        return f\"Error: An I/O error occurred while reading the file at path {file_path}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"String has been written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about a use case.\n",
    "Then you predict all scenarios that can happen in this use case.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- Stay close to the details described in the flow.\n",
    "- Focus on important and likely scenarios, important scenario is the scenario that users are more likely to encounter it. \n",
    "- Minimize the appearance of rare scenarios. \n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Test scenarios should be derived from cohesive sequences of steps that represent meaningful user interactions, rather than isolated steps.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_path = r\"C:\\Users\\congc\\Desktop\\GPT\\Dissertation-GPT\\dataset\\SpecificationData\\MatchaEnglishWebsite\"\n",
    "# save_path = r\"D:\\Dissertation-GPT\\evaluate\\prove\\Matcha\\ts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name_list = []\n",
    "project_name = os.path.basename(usecase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Add new topic', 'AddLesson', 'AddQuestion ', 'EditTest', 'Flashcard', 'Handbook', 'Learning days history', 'ListenToPronunciation', 'Login', 'Registry', 'Review test', 'WordSortingTest']\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(usecase_path):\n",
    "    usecase_name_list.append (filename.split(\".txt\")[0])\n",
    "print(usecase_name_list)\n",
    "usecase_name_list=[\"AddQuestion \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddQuestion \n",
      "gpt_response \n",
      "1. Successful Question Creation\n",
      "2. Access Lesson Management from Homepage\n",
      "3. Skip Compulsory Field \"Sentence\"\n",
      "4. Skip Compulsory Field \"Correct Answer\"\n",
      "5. Skip Compulsory Field \"Other Option 1\"\n",
      "6. Skip Compulsory Field \"Other Option 2\"\n",
      "7. Skip Compulsory Field \"Other Option 3\"\n",
      "8. Cancel Question Creation and Choose to Stay\n",
      "9. Enter Invalid Sentence\n",
      "10. Leave Without Saving by Browser Back Button\n",
      "11. Cancel Question Creation and Confirm Cancel\n"
     ]
    }
   ],
   "source": [
    "for usecase_name in usecase_name_list:\n",
    "    print(usecase_name)\n",
    "    usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "    prompt_all = read_file_content(usecase_directlink)\n",
    "    promptSYSTEM_PROMPT = [\n",
    "        { \"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        { \"role\": \"user\", \"content\": prompt_all}\n",
    "    ]\n",
    "    gpt_response = ask(promptSYSTEM_PROMPT, client, model)    \n",
    "    print(\"gpt_response \\n\" +gpt_response)\n",
    "    # write_string_to_file(os.path.join(save_path,f\"{usecase_name}.txt\"), gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
