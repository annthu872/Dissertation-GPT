{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def askJSON(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"String has been written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt\n",
    "MAIN_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow of a use case.\n",
    "Then you predict all scenarios that can happen in this flow.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- Stay close to the details described in the flow.\n",
    "- Choose important cases to generate, important scenario is the scenario that users are more likely to encounter it.\n",
    "- Limit the appearance of scenarios that are hard to happen. \n",
    "- A scenario encompasses a whole function, not just verifying individual steps.\n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Test scenarios should be derived from cohesive sequences of steps that represent meaningful user interactions, rather than isolated steps.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "SUB_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow and one alternative or exception flow of a use case.\n",
    "Then you predict all scenarios that can lead user from the main flow to change to the alternative or exception flow mentioned for creating test cases.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- A scenario encompasses a whole function, not just verifying individual steps.\n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- Test scenarios should be derived from cohesive sequences of steps that represent meaningful user interactions, rather than isolated steps.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "- Do not generate scenarios with user analysis. (Example: User accidentally do A and user intentionally do A is the same scenario, so do not consider about \"accidentally\" or \"intentionally\" in scenario)\n",
    "- Do not choose another option that is not chosen by the flow, eventhough it is mentioned (Example: A pop up with OK and Cancel, the flow only has step choose OK. Do not generate scenario that press Cancel)\n",
    "- Do not generate scenario to test only the main flow.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "EXTRACT_CONDITION_SYSTEM_PROMPT=\"\"\"\n",
    "Given use case flows of a feature.\n",
    "Your task is to identify all the interactive elements within the feature. \n",
    "For each interactive element:\n",
    "Identify what type of that element (button,buttons, icon,scroll, text field,text area, tab, radio buttons, menu, combobox, sliders, switches, dialog, link, form,rating, filter).\n",
    "Identify all the conditions mentioned in the use case of that element that would make the element valid and the conditions that would make the element invalid based on the description of the use case flow.\n",
    "Do not arbitrarily create additional conditions that not mention in the use case flow.\n",
    "Return the element extracted in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\"Name of interactive element\": {\"condition\": {valid:\"conditions that make element valid\", invalid: \"conditions that make element invalid\"}, \"type\": \"element type\"}}\n",
    "Examples of output json format template: \n",
    "{\"Username\": {\"condition\": {\"valid\": \"must be over 8 characters and below 30 characters, must be entered\", racter, empty\"}, \"type\": \"text field\"}}\n",
    "{\"Search button\": {\"condition\": {\"valid\": \"\", \"invalid\": \"\"}, \"type\": \"text field\"}}\n",
    "\"\"\"\n",
    "\n",
    "GEN_SCENARIO_FOR_CONDITION_PROMPT=\"\"\"\n",
    "Given a list of interaction element for input value, their extracted conditions and use c\"invalid\": \"below 8 characters, over 30 chaase specification.\n",
    "For each given invalid condition:\n",
    "- Generate a test scenario that test that condition.\n",
    "Do not generate test scenario to test element/condition that not mention in the given element list.\n",
    "Do not generate test scenario to test valid conditions.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "FILTER_SYSTEM_PROMPT=\"\"\"\n",
    "Your task is to read all of this scenarios generation from many sources.\n",
    "Then you remove all the same copies because there are many duplicate scenarios in meanings inside the input.\n",
    "Make sure every scenarios in the response is unique.\n",
    "Return scenario name only.\n",
    "\"\"\"\n",
    "\n",
    "# Identify what type of that element (button,buttons, icon,scroll, text field, radio buttons, menu, menu for navigation,menu for opening dialog or another menu, menu for filter, sliders, switches, dialog, link, form,rating, filter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name = \"WordSortingTest\"\n",
    "project_name = \"MatchaEnglishWebsite\"\n",
    "main_flow_prompt = \"\"\"\n",
    "basic flow:\n",
    "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
    "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
    "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
    "Step 4: If the answer is correct, the pop up screen is green.\n",
    "\"\"\"\n",
    "prompt_all = \"\"\"\n",
    "basic flow:\n",
    "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
    "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
    "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
    "Step 4: If the answer is correct, the pop up screen is green.\n",
    "\n",
    "Alternative flow 1: Learner chooses wrong answer\n",
    "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
    "\n",
    "\"\"\"\n",
    "alt_prompt=[\n",
    "\"\"\"Alternative flow 1: Learner chooses wrong answer\n",
    "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\"\"\"]\n",
    "exc_prompt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "basic flow:\n",
      "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
      "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
      "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
      "Step 4: If the answer is correct, the pop up screen is green.\n",
      "\n",
      "\n",
      "Alternative flow 1: Learner chooses wrong answer\n",
      "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "{}\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "filter_gpt_response \n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario\n",
      "4. No Answer Submitted Scenario\n",
      "String has been written to D:\\GPT-testing\\ResultSet\\5.15\\MatchaEnglishWebsite\\WordSortingTest-1.txt\n"
     ]
    }
   ],
   "source": [
    "promptMainScenario = [\n",
    "    { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "  ]\n",
    "main_gpt_response = ask(promptMainScenario, client, model)\n",
    "filtercontent = main_gpt_response\n",
    "print(filtercontent)\n",
    "for alt in alt_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+alt)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "for exc in exc_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+exc}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+exc)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "\n",
    "promptExtractCondition = [\n",
    "    { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": prompt_all}\n",
    "  ]\n",
    "gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "full_elements = json.loads(gpt_response)\n",
    "condition_element = {key: value for key, value in full_elements.items() \n",
    "                              if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "print(condition_element)\n",
    "\n",
    "if(len(condition_element)!= 0):\n",
    "  promptScenarioForCondition = [\n",
    "      { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "    ]\n",
    "  condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "  print(condition_scenario_response)\n",
    "  filtercontent += \"\\n\" + condition_scenario_response\n",
    "\n",
    "print(filtercontent)\n",
    "\n",
    "promptFilter = [\n",
    "    { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": filtercontent}\n",
    "  ]\n",
    "filter_gpt_response = ask(promptFilter, client, model)\n",
    "print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "write_string_to_file(f\"D:\\\\GPT-testing\\\\ResultSet\\\\5.15\\\\{project_name}\\\\{usecase_name}-1.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "basic flow:\n",
      "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
      "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
      "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
      "Step 4: If the answer is correct, the pop up screen is green.\n",
      "\n",
      "\n",
      "Alternative flow 1: Learner chooses wrong answer\n",
      "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "{}\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario (if applicable, depending on system design)\n",
      "4. No Answer Submitted Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "filter_gpt_response \n",
      "1. Correct Answer Scenario\n",
      "2. Incorrect Answer Scenario\n",
      "3. Partially Correct Answer Scenario\n",
      "4. No Answer Submitted Scenario\n",
      "String has been written to D:\\GPT-testing\\ResultSet\\5.15\\MatchaEnglishWebsite\\WordSortingTest-2.txt\n"
     ]
    }
   ],
   "source": [
    "promptMainScenario = [\n",
    "    { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "  ]\n",
    "main_gpt_response = ask(promptMainScenario, client, model)\n",
    "filtercontent = main_gpt_response\n",
    "print(filtercontent)\n",
    "for alt in alt_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+alt)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "for exc in exc_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+exc}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+exc)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "\n",
    "promptExtractCondition = [\n",
    "    { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": prompt_all}\n",
    "  ]\n",
    "gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "full_elements = json.loads(gpt_response)\n",
    "condition_element = {key: value for key, value in full_elements.items() \n",
    "                              if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "print(condition_element)\n",
    "\n",
    "if(len(condition_element)!= 0):\n",
    "  promptScenarioForCondition = [\n",
    "      { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "    ]\n",
    "  condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "  print(condition_scenario_response)\n",
    "  filtercontent += \"\\n\" + condition_scenario_response\n",
    "\n",
    "print(filtercontent)\n",
    "\n",
    "promptFilter = [\n",
    "    { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": filtercontent}\n",
    "  ]\n",
    "filter_gpt_response = ask(promptFilter, client, model)\n",
    "print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "write_string_to_file(f\"D:\\\\GPT-testing\\\\ResultSet\\\\5.15\\\\{project_name}\\\\{usecase_name}-2.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "basic flow:\n",
      "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
      "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
      "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
      "Step 4: If the answer is correct, the pop up screen is green.\n",
      "\n",
      "\n",
      "Alternative flow 1: Learner chooses wrong answer\n",
      "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "Based on the provided information, here are the scenarios that can lead a user from the main flow to the alternative or exception flow:\n",
      "\n",
      "1. Scenario: Learner chooses wrong answer\n",
      "{}\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "Based on the provided information, here are the scenarios that can lead a user from the main flow to the alternative or exception flow:\n",
      "\n",
      "1. Scenario: Learner chooses wrong answer\n",
      "filter_gpt_response \n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "5. Learner chooses wrong answer\n",
      "String has been written to D:\\GPT-testing\\ResultSet\\5.15\\MatchaEnglishWebsite\\WordSortingTest-3.txt\n"
     ]
    }
   ],
   "source": [
    "promptMainScenario = [\n",
    "    { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "  ]\n",
    "main_gpt_response = ask(promptMainScenario, client, model)\n",
    "filtercontent = main_gpt_response\n",
    "print(filtercontent)\n",
    "for alt in alt_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+alt)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "for exc in exc_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+exc}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+exc)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "\n",
    "promptExtractCondition = [\n",
    "    { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": prompt_all}\n",
    "  ]\n",
    "gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "full_elements = json.loads(gpt_response)\n",
    "condition_element = {key: value for key, value in full_elements.items() \n",
    "                              if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "print(condition_element)\n",
    "\n",
    "if(len(condition_element)!= 0):\n",
    "  promptScenarioForCondition = [\n",
    "      { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "    ]\n",
    "  condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "  print(condition_scenario_response)\n",
    "  filtercontent += \"\\n\" + condition_scenario_response\n",
    "\n",
    "print(filtercontent)\n",
    "\n",
    "promptFilter = [\n",
    "    { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": filtercontent}\n",
    "  ]\n",
    "filter_gpt_response = ask(promptFilter, client, model)\n",
    "print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "write_string_to_file(f\"D:\\\\GPT-testing\\\\ResultSet\\\\5.15\\\\{project_name}\\\\{usecase_name}-3.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Partial Answer Submission Scenario\n",
      "4. No Answer Submission Scenario\n",
      "\n",
      "basic flow:\n",
      "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
      "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
      "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
      "Step 4: If the answer is correct, the pop up screen is green.\n",
      "\n",
      "\n",
      "Alternative flow 1: Learner chooses wrong answer\n",
      "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Partial Answer Submission Scenario\n",
      "4. No Answer Submission Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "{}\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Partial Answer Submission Scenario\n",
      "4. No Answer Submission Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "filter_gpt_response \n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Partial Answer Submission Scenario\n",
      "4. No Answer Submission Scenario\n",
      "String has been written to D:\\GPT-testing\\ResultSet\\5.15\\MatchaEnglishWebsite\\WordSortingTest-4.txt\n"
     ]
    }
   ],
   "source": [
    "promptMainScenario = [\n",
    "    { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "  ]\n",
    "main_gpt_response = ask(promptMainScenario, client, model)\n",
    "filtercontent = main_gpt_response\n",
    "print(filtercontent)\n",
    "for alt in alt_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+alt)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "for exc in exc_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+exc}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+exc)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "\n",
    "promptExtractCondition = [\n",
    "    { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": prompt_all}\n",
    "  ]\n",
    "gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "full_elements = json.loads(gpt_response)\n",
    "condition_element = {key: value for key, value in full_elements.items() \n",
    "                              if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "print(condition_element)\n",
    "\n",
    "if(len(condition_element)!= 0):\n",
    "  promptScenarioForCondition = [\n",
    "      { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "    ]\n",
    "  condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "  print(condition_scenario_response)\n",
    "  filtercontent += \"\\n\" + condition_scenario_response\n",
    "\n",
    "print(filtercontent)\n",
    "\n",
    "promptFilter = [\n",
    "    { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": filtercontent}\n",
    "  ]\n",
    "filter_gpt_response = ask(promptFilter, client, model)\n",
    "print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "write_string_to_file(f\"D:\\\\GPT-testing\\\\ResultSet\\\\5.15\\\\{project_name}\\\\{usecase_name}-4.txt\", filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "basic flow:\n",
      "Step 1: The screen displays a question or the meaning of a vocabulary or an image showing the word and those letters of that vocabulary arranged randomly, not in correct order. \n",
      "Step 2: Learner arrange letters by clicking on letters button in correct order and press enter after finish arranging.\n",
      "Step 3: The system compares user answer and the right English vocabulary for the question.\n",
      "Step 4: If the answer is correct, the pop up screen is green.\n",
      "\n",
      "\n",
      "Alternative flow 1: Learner chooses wrong answer\n",
      "At step 4 of basic flow: If the answer is wrong, the pop up screen is red.\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "{}\n",
      "Based on the provided flow, here are the scenarios that can happen:\n",
      "\n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "\n",
      "1. Learner Chooses Wrong Answer Scenario\n",
      "filter_gpt_response \n",
      "1. Correct Answer Submission Scenario\n",
      "2. Incorrect Answer Submission Scenario\n",
      "3. Submission with Missing Letters Scenario\n",
      "4. Submission with Extra Letters Scenario\n",
      "String has been written to D:\\GPT-testing\\ResultSet\\5.15\\MatchaEnglishWebsite\\WordSortingTest-5.txt\n"
     ]
    }
   ],
   "source": [
    "promptMainScenario = [\n",
    "    { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "  ]\n",
    "main_gpt_response = ask(promptMainScenario, client, model)\n",
    "filtercontent = main_gpt_response\n",
    "print(filtercontent)\n",
    "for alt in alt_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+alt)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "for exc in exc_prompt:\n",
    "\n",
    "  promptSubScenario = [\n",
    "      { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+exc}\n",
    "    ]\n",
    "  print(main_flow_prompt +\"\\n\\n\"+exc)\n",
    "  sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "  filtercontent+=  \"\\n\\n\" + sub_gpt_response\n",
    "  print(filtercontent)\n",
    "\n",
    "\n",
    "\n",
    "promptExtractCondition = [\n",
    "    { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": prompt_all}\n",
    "  ]\n",
    "gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "full_elements = json.loads(gpt_response)\n",
    "condition_element = {key: value for key, value in full_elements.items() \n",
    "                              if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "print(condition_element)\n",
    "\n",
    "if(len(condition_element)!= 0):\n",
    "  promptScenarioForCondition = [\n",
    "      { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "      { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "    ]\n",
    "  condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "  print(condition_scenario_response)\n",
    "  filtercontent += \"\\n\" + condition_scenario_response\n",
    "\n",
    "print(filtercontent)\n",
    "\n",
    "promptFilter = [\n",
    "    { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "    { \"role\": \"user\", \"content\": filtercontent}\n",
    "  ]\n",
    "filter_gpt_response = ask(promptFilter, client, model)\n",
    "print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "write_string_to_file(f\"D:\\\\GPT-testing\\\\ResultSet\\\\5.15\\\\{project_name}\\\\{usecase_name}-5.txt\", filter_gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
