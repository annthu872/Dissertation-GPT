{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_5=\"\"\"\n",
    "I want you to act as a software tester.\n",
    "Your task is to read the test scenario's name and the corresponding use case specification to predict the goal of the test scenario.\n",
    "Also, generate a test case includes test steps based on those information to cover the test scenario.\n",
    "Rules for generating test case:\n",
    "- If that test case needs login as a specific role to proceed, do not put login at step 1. Instead, put the login as that specific role in the pre condition.\n",
    "- If in the use case specification do not describe how to go to the page to do the test case, do not put it in the step. Instead, tell the tester that they need to be in that page in the pre condition.\n",
    "- If there are navigation steps in the use case specification, keep it. Do not put it in the precondition or combine it with other steps.\n",
    "- If the test case need to be repeated to test with different order, data or case, seperate them to be distinct test cases.\n",
    "- Generate only test cases that the test scenario's name mentioned. Do not try to cover everything in the use case specification if the test scenario's name does not describe it. \n",
    "- Write the test step clearly, do not write something like \"Follow steps from the main flow\".\n",
    "- If the scenario is about testing the displation and there is no flow directly cover that scenario, use only the basic (or main) flow to test it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Scenario: Checkout Process with System Error During Save\n",
    "\n",
    "Feedback\n",
    "\n",
    "Description\n",
    "This use case allows the user to give feedback about the items. Based on the feedback the shop owner operates.\n",
    "\n",
    "Basic Flow\n",
    "This use case starts when the actor finishes checkout.\n",
    "1.The system asks the user for details.\n",
    "2.The user enters the requested details.\n",
    "3.The entered details are saved and the user is acknowledged.\n",
    "\n",
    "Pre-Conditions\n",
    "The actor has finished checkout successfully.\n",
    "\n",
    "Post-Conditions \n",
    "If the use case is successful, The feedback will be sent to the shop owner.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Test Case: Verify System Error Handling During Feedback Submission After Checkout\n",
      "\n",
      "#### Goal:\n",
      "To ensure that the system gracefully handles errors that occur while saving user feedback after the checkout process.\n",
      "\n",
      "#### Pre-Conditions:\n",
      "- The tester must have completed the checkout process successfully.\n",
      "- The tester needs to be on the feedback submission page.\n",
      "\n",
      "#### Test Steps:\n",
      "1. Enter valid details into the feedback form as requested by the system.\n",
      "2. Simulate a system error during the save operation (This step may require coordination with a developer to induce a system error condition such as a database write failure).\n",
      "3. Observe the system's response to the error condition.\n",
      "\n",
      "#### Expected Result:\n",
      "The system should display an appropriate error message indicating that the feedback could not be saved due to a system error. The user should be given options to retry the submission or contact support.\n",
      "\n",
      "#### Note:\n",
      "This test case focuses on the system's ability to handle errors during the feedback submission process post-checkout. It does not cover the successful submission of feedback or the checkout process itself.\n"
     ]
    }
   ],
   "source": [
    "promptTestCase = [\n",
    "    { \"role\": \"system\", \"content\": SYSTEM_PROMPT_5},\n",
    "    { \"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "gpt_response = ask(promptTestCase, client, model)\n",
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Test Case: System Error During Save in Feedback Submission\n",
      "\n",
      "#### Goal:\n",
      "To verify the system's behavior when an error occurs during the saving process of feedback submission after checkout.\n",
      "\n",
      "#### Pre-Conditions:\n",
      "- The tester must have successfully completed the checkout process.\n",
      "- The tester needs to be on the feedback submission page.\n",
      "\n",
      "#### Test Steps:\n",
      "1. Observe the system asking for feedback details.\n",
      "2. Enter valid feedback details into the requested fields.\n",
      "3. Simulate a system error during the save operation (This step may require coordination with a developer or the use of a testing tool to induce a system error).\n",
      "4. Observe the system's response to the error.\n",
      "\n",
      "#### Expected Result:\n",
      "The system should handle the error gracefully, possibly by displaying an error message to the user indicating that the feedback could not be saved and suggesting the user to try again later.\n"
     ]
    }
   ],
   "source": [
    "promptTestCase = [\n",
    "    { \"role\": \"system\", \"content\": SYSTEM_PROMPT_5},\n",
    "    { \"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "gpt_response = ask(promptTestCase, client, model)\n",
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Test Case: Verify System Error Handling During Feedback Submission After Checkout\n",
      "\n",
      "#### Objective:\n",
      "To ensure that the system gracefully handles errors during the feedback submission process after a successful checkout.\n",
      "\n",
      "#### Pre-Conditions:\n",
      "- The tester has successfully completed the checkout process.\n",
      "- The tester is on the feedback submission page.\n",
      "\n",
      "#### Test Steps:\n",
      "1. Enter valid details into the feedback form as requested by the system.\n",
      "2. Simulate a system error during the save operation (This step might require coordination with a developer to induce a system error or use a testing environment where errors can be simulated).\n",
      "3. Observe the system's response to the error.\n",
      "\n",
      "#### Expected Result:\n",
      "- The system should display an appropriate error message indicating that the feedback could not be saved due to a system error.\n",
      "- The system should offer the user options to retry the submission or save the feedback to attempt submission at a later time.\n",
      "\n",
      "#### Post-Conditions:\n",
      "- Feedback is not sent to the shop owner due to the system error.\n",
      "- The system logs the error for further investigation by the technical team.\n"
     ]
    }
   ],
   "source": [
    "promptTestCase = [\n",
    "    { \"role\": \"system\", \"content\": SYSTEM_PROMPT_5},\n",
    "    { \"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "gpt_response = ask(promptTestCase, client, model)\n",
    "print(gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
