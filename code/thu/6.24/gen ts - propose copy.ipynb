{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "GPT_MODEL_4 = \"gpt-4-0125-preview\"\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "model = GPT_MODEL_4\n",
    "\n",
    "def ask(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def askJSON(prompt, client, model, temperature = 0):\n",
    "    response = None\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=prompt,\n",
    "      temperature=temperature,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    # Detect the encoding\n",
    "    with open(file_path, 'rb') as file:\n",
    "        raw_data = file.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        encoding = result['encoding']\n",
    "    \n",
    "    # Read the file with the detected encoding\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: The file at path {file_path} was not found.\"\n",
    "    except UnicodeDecodeError:\n",
    "        return f\"Error: The file at path {file_path} cannot be decoded with the {encoding} encoding.\"\n",
    "    except IOError:\n",
    "        return f\"Error: An I/O error occurred while reading the file at path {file_path}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_string_to_file(filename, content):\n",
    "    try:\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        print(f\"String has been written to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing to the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt\n",
    "MAIN_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow of a use case.\n",
    "Then you predict all scenarios that can happen in this flow.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- Focus on important and likely scenarios, important scenario is the scenario that users are more likely to encounter it. \n",
    "- Minimize the appearance of rare scenarios. \n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "- Take account of the preconditions (if mentioned) and create scenario to test if precondtition affects main flow.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "SUB_FLOW_SYSTEM_PROMPT=\"\"\"\n",
    "I want you to act as software tester.\n",
    "Your task is to read this information about one main flow and one alternative or exception flow of a use case.\n",
    "Then you predict all scenarios that can lead user from the main flow to change to the alternative or exception flow mentioned for creating test cases.\n",
    "\n",
    "Rules to predict scenarios:\n",
    "- If there is no other action in the flow beside clicking or there is no condition to vary the user's actions, that flow has one scenario only.\n",
    "- A scenario often refers to a specific sequence of events or user actions that could potentially lead to a change in how the application behaves or responds.\n",
    "- A scenario should cover from the first step to the final step in the flow, the start or the result of the scenario could be different.\n",
    "- You cannot separate parts of a flow to be a scenario (Example: predict multiple scenarios for a flow by dividing steps into parts) because each scenarios should be independent and require a complete flow to proceed.\n",
    "- Do not generate scenarios with user analysis. (Example: User accidentally do A and user intentionally do A is the same scenario, so do not consider about \"accidentally\" or \"intentionally\" in scenario)\n",
    "- Do not choose another option that is not chosen by the flow, eventhough it is mentioned (Example: A pop up with OK and Cancel, the flow only has step choose OK. Do not generate scenario that press Cancel)\n",
    "- Do not generate scenario to test only the main flow.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "\n",
    "EXTRACT_CONDITION_SYSTEM_PROMPT=\"\"\"\n",
    "Given use case flows of a feature.\n",
    "Your task is to identify all the interactive elements within the feature. \n",
    "For each interactive element:\n",
    "Identify what type of that element (button,buttons, icon,scroll, text field,text area, tab, radio buttons, menu, combobox, sliders, switches, dialog, link, form,rating, filter).\n",
    "Identify all the conditions mentioned in the use case of that element that would make the element valid and the conditions that would make the element invalid based on the description of the use case flow.\n",
    "Do not arbitrarily create additional conditions that not mention in the use case flow.\n",
    "Return the element extracted in json format.\n",
    "The JSON format should follow the following structure:\n",
    "{\"Name of interactive element\": {\"condition\": {valid:\"conditions that make element valid\", invalid: \"conditions that make element invalid\"}, \"type\": \"element type\"}}\n",
    "Examples of output json format template: \n",
    "{\"Username\": {\"condition\": {\"valid\": \"must be over 8 characters and below 30 characters, must be entered\", racter, empty\"}, \"type\": \"text field\"}}\n",
    "{\"Search button\": {\"condition\": {\"valid\": \"\", \"invalid\": \"\"}, \"type\": \"text field\"}}\n",
    "\"\"\"\n",
    "\n",
    "GEN_SCENARIO_FOR_CONDITION_PROMPT=\"\"\"\n",
    "Given a list of interaction element for input value, their extracted conditions and corresponding use case.\n",
    "For each given invalid condition:\n",
    "- Generate a test scenario that test that condition.\n",
    "Do not generate test scenario to test element/condition that not mention in the given element list.\n",
    "Do not generate test scenario to test valid conditions.\n",
    "I only need scenarios's name for the output, I do not need the steps to go with it.\n",
    "\"\"\"\n",
    "FILTER_SYSTEM_PROMPT=\"\"\"\n",
    "You will be provide with a use case and a list of test scenario.\n",
    "Based on information in the use case flow, define what test scenario is necessary to test the use case and remove duplicate test scenarios.\n",
    "Remove test case test system load error that not be mentioned in use case such as: Load Failure, System Error,Network Error,Non-Existent ...(and use case not mentioned these flow)\n",
    "Make sure test scenarios filted cover all the flow of use case and every scenarios in the response is unique.\n",
    "Return scenario name only.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_extract_flow =\"\"\"\n",
    "Given use case specification flows.\n",
    "Extract all the contents of flows into json format. \n",
    "If the use case's main scenario has a name other than 'Main flow', rename it to Main flow.\n",
    "Other flows just keep the same name.\n",
    "Skip all the information that not describe the flow action of use case(description, precondition,postcondition,...).\n",
    "Return the flows in json format.\n",
    "{\n",
    "\"Flow name\":[ contents of corresponding flow],\n",
    "}\n",
    "For example: \n",
    "{\n",
    "  \"Main flow\": [\n",
    "    \"Step 1: Learner fills in the username field by a valid username that has been registered\",\n",
    "    \"Step 2: Learner fills in the password field by the correct password for the corresponding username\",\n",
    "    \"Step 3: Learner press \\\"Login\\\" button\",\n",
    "    \"Step 4: System redirects learner to Home page\"\n",
    "  ],\n",
    "  \"Alternative flow 1: Login by email\": [\n",
    "    \"At step 1 of the basic flow: Learner fills in the username field by a valid email that has been registered\",\n",
    "    \"Step 2: Learner fills in the password field by the correct password for the corresponding email\",\n",
    "    \"Go back to step 3 in the basic flow and continue with the steps from step 3\"\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_path = r\"D:\\Dissertation-GPT\\dataset\\SpecificationData\\Hotel Management System\"\n",
    "save_path = r\"D:\\Dissertation-GPT\\evaluate\\ResultSet\\6.24\\shopee 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase_name_list = []\n",
    "project_name = os.path.basename(usecase_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(usecase_path):\n",
    "#     usecase_name_list.append (filename.split(\".txt\")[0])\n",
    "usecase_name_list = [\"Update Booking\"]\n",
    "# print(usecase_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Booking\n",
      "Alternative path 1:\n",
      "At step 3: If the booking doesnâ€™t exist, show a NOT FOUND message.\n",
      "\n",
      "\n",
      "Alternative path 2:\n",
      "At step 6: Payment must be computed first then update can be done.\n",
      "\n",
      "\n",
      "Alternative path 3:\n",
      "At step 8: If invalid field is entered, show invalid field entered for every individual error.\n",
      "\n",
      "\n",
      "Alternative path 4:\n",
      "At step 10: In case of any error show a failure message.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for usecase_name in usecase_name_list:\n",
    "#     print(usecase_name)\n",
    "#     usecase_directlink = os.path.join(usecase_path,usecase_name+\".txt\")\n",
    "#     prompt_all = read_file_content(usecase_directlink)\n",
    "#     promptExtractFlow = [\n",
    "#     { \"role\": \"system\", \"content\": SYSTEM_PROMPT_extract_flow},\n",
    "#     { \"role\": \"user\", \"content\": prompt_all}\n",
    "#     ]\n",
    "#     gpt_response = askJSON(promptExtractFlow, client, model)\n",
    "#     flows = json.loads(gpt_response)\n",
    "#     main_flow = flows['Main flow']\n",
    "#     alt_prompt=[]\n",
    "#     main_flow_prompt = \"Main flow:\"\n",
    "#     for i in main_flow:\n",
    "#         main_flow_prompt+=\"\\n\"+i\n",
    "#     for key,value in flows.items():\n",
    "#         if(key not in 'Main flow'):\n",
    "#             flow = key+ \":\" \n",
    "#             for i in value:\n",
    "#                 flow += \"\\n\"+ i\n",
    "#             alt_prompt.append(flow)\n",
    "    \n",
    "#     for alt in alt_prompt:\n",
    "#         print(alt)\n",
    "#         print\n",
    "#     # for i in range(1,4):\n",
    "#     #     print(\"TIME \"+str(i))\n",
    "#     #     print(usecase_directlink)\n",
    "#     #     filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "#     #     promptExtractCondition = [\n",
    "#     #         { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "#     #         { \"role\": \"user\", \"content\": prompt_all}\n",
    "#     #         ]\n",
    "#     #     gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "#     #     full_elements = json.loads(gpt_response)\n",
    "#     #     condition_element = {key: value for key, value in full_elements.items() \n",
    "#     #                                     if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "#     #     print(condition_element)\n",
    "#     #     filtercontent = \"\"\n",
    "#     #     if(len(condition_element)!= 0):\n",
    "#     #         promptScenarioForCondition = [\n",
    "#     #             { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "#     #             { \"role\": \"user\", \"content\": prompt_all + '\\nElement:' + str(condition_element)}\n",
    "#     #         ]\n",
    "#     #         condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "#     #         filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "#     #         print(condition_scenario_response)\n",
    "#     #     promptMainScenario = [\n",
    "#     #         { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "#     #         { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "#     #         ]\n",
    "#     #     main_gpt_response = ask(promptMainScenario, client, model)\n",
    "#     #     filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "#     #     print(main_flow_prompt)\n",
    "#     #     print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "#     #     for alt in alt_prompt:\n",
    "#     #         promptSubScenario = [\n",
    "#     #             { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "#     #             { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "#     #         ]\n",
    "#     #         sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "#     #         print(alt)\n",
    "#     #         print(\"\\nsub_gpt_response:\"+sub_gpt_response +\"\\n\")\n",
    "#     #         filtercontent+=  \"\\n\" + sub_gpt_response\n",
    "            \n",
    "#     #     print(\"filtercontent: \"+ filtercontent)\n",
    "#     #     filtercontent1 += filtercontent\n",
    "#     #     promptFilter = [\n",
    "#     #         { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "#     #         { \"role\": \"user\", \"content\": filtercontent1}\n",
    "#     #         ]\n",
    "#     #     filter_gpt_response = ask(promptFilter, client, model)\n",
    "#     #     print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "#     #     write_string_to_file(os.path.join(save_path,f\"{usecase_name}-{i}.txt\"), filter_gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(1,4):\n",
    "#   print(\"TIME \"+str(i))\n",
    "  \n",
    "#   filtercontent1 = \"Use case: \" + prompt_all +\"\\n Test scenarios: \\n\"\n",
    "#   promptExtractCondition = [\n",
    "#       { \"role\": \"system\", \"content\": EXTRACT_CONDITION_SYSTEM_PROMPT},\n",
    "#       { \"role\": \"user\", \"content\": prompt_all}\n",
    "#     ]\n",
    "#   gpt_response = askJSON(promptExtractCondition, client, model)\n",
    "#   full_elements = json.loads(gpt_response)\n",
    "#   condition_element = {key: value for key, value in full_elements.items() \n",
    "#                                 if (value['type'] in ['text field','text area'] and (value['condition']['valid'] or value['condition']['invalid'] ))}\n",
    "#   print(condition_element)\n",
    "#   filtercontent = \"\"\n",
    "#   if(len(condition_element)!= 0):\n",
    "#     promptScenarioForCondition = [\n",
    "#         { \"role\": \"system\", \"content\": GEN_SCENARIO_FOR_CONDITION_PROMPT},\n",
    "#         { \"role\": \"user\", \"content\": prompt_all + '\\n Element:' + str(condition_element)}\n",
    "#       ]\n",
    "#     condition_scenario_response = ask(promptScenarioForCondition, client, model)\n",
    "#     filtercontent = \"\\nCondition Scenario: \" + condition_scenario_response\n",
    "#     print(condition_scenario_response)\n",
    "#   promptMainScenario = [\n",
    "#       { \"role\": \"system\", \"content\": MAIN_FLOW_SYSTEM_PROMPT},\n",
    "#       { \"role\": \"user\", \"content\": main_flow_prompt}\n",
    "#     ]\n",
    "#   main_gpt_response = ask(promptMainScenario, client, model)\n",
    "#   filtercontent += \"\\n Flow cover scenarios: \" +  main_gpt_response\n",
    "#   print(main_flow_prompt)\n",
    "#   print(\"\\nmain_gpt_response:\"+main_gpt_response+\"\\n\")\n",
    "#   for alt in alt_prompt:\n",
    "#     promptSubScenario = [\n",
    "#         { \"role\": \"system\", \"content\": SUB_FLOW_SYSTEM_PROMPT},\n",
    "#         { \"role\": \"user\", \"content\": main_flow_prompt +\"\\n\\n\"+alt}\n",
    "#       ]\n",
    "#     sub_gpt_response = ask(promptSubScenario, client, model)\n",
    "#     print(alt)\n",
    "#     print(\"\\nsub_gpt_response:\"+sub_gpt_response +\"\\n\")\n",
    "#     filtercontent+=  \"\\n\" + sub_gpt_response\n",
    "    \n",
    "#   print(\"filtercontent: \"+ filtercontent)\n",
    "#   filtercontent1 += filtercontent\n",
    "#   promptFilter = [\n",
    "#       { \"role\": \"system\", \"content\": FILTER_SYSTEM_PROMPT},\n",
    "#       { \"role\": \"user\", \"content\": filtercontent1}\n",
    "#     ]\n",
    "#   filter_gpt_response = ask(promptFilter, client, model)\n",
    "#   print(\"filter_gpt_response \\n\" +filter_gpt_response)\n",
    "#   write_string_to_file(f\"{usecase_name}-{i}.txt\", filter_gpt_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
